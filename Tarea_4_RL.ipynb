{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmDI-h7cI0tI"
      },
      "source": [
        "# Tarea 4: Aprendizaje por Refuerzo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKOCZlhUgXVK"
      },
      "source": [
        "#Objetivos\n",
        "\n",
        " - Entender conceptos y técnicas básicas de Reinforcement Learning.\n",
        "\n",
        " -Implementar RL mediante políticas en entornos controlados simples.\n",
        "\n",
        "## 1.\tEl Problema\n",
        "Un poste está unido mediante una articulación no accionada a un carro, que se mueve a lo largo de una vía sin fricción. El péndulo se coloca en posición vertical sobre el carro y el objetivo es equilibrar el poste aplicando fuerzas en la dirección izquierda y derecha sobre el carro\n",
        "\n",
        "\n",
        "![Cartpole environment](https://raw.githubusercontent.com/tensorflow/agents/master/docs/tutorials/images/cartpole.png)\n",
        "\n",
        "###1.1.\tDesafío\n",
        "Entrenar un agente de mediante aprendizaje por refuerzo, ajustando sus hiper parámetros, que sea capaz de aprender una política que le permita mantener el equilibrio por más de 40 segundos.\n",
        "\n",
        "###1.2.\tSalida\n",
        "Como resultado, se debe entregar el conjunto de hiper parámetros utilizados, justificando su elección, así como un video del carro sosteniendo el equilibrio.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Dependencias!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNrNXKI7bINP"
      },
      "source": [
        "Si no ha instalado las siguientes dependencias, ejecute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEHR2Ui-lo8O"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pyglet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sMitx5qSgJk1"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (reverb.py, line 64)",
          "output_type": "error",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "\u001b[0m  File \u001b[1;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:3526\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
            "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 11\u001b[0;36m\n\u001b[0;31m    import reverb\u001b[0;36m\n",
            "\u001b[0;36m  File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/reverb.py:64\u001b[0;36m\u001b[0m\n\u001b[0;31m    raise TypeError, 'Regexp cannot be negated'\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import reverb\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6HsdS5GbSjd"
      },
      "outputs": [],
      "source": [
        "# Configure una pantalla virtual para representar entornos de OpenAI gym.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NspmzG4nP3b9"
      },
      "outputs": [],
      "source": [
        "tf.version.VERSION #debe dar version superior a 2.14.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "## Hiperparámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC1kNrOsLSIZ"
      },
      "outputs": [],
      "source": [
        "num_iterations = 1 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 1  # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration =   1# @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 1  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 1  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "log_interval = 1  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 1  # @param {type:\"integer\"}\n",
        "eval_interval = 1  # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Ambiente\n",
        "\n",
        "En el Aprendizaje por Refuerzo (RL), un entorno representa la tarea o problema a resolver.\n",
        "\n",
        "Cargue el entorno CartPole desde la suite OpenAI Gym."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYEz-S9gEv2-"
      },
      "outputs": [],
      "source": [
        "env_name = 'CartPole-v0'\n",
        "env = suite_gym.load(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIHYVBkuvPNw"
      },
      "source": [
        "Puede renderizar este entorno para ver cómo se ve. Un poste que se balancea libremente está sujeto a un carro.\n",
        "\n",
        "El objetivo es mover el carro hacia la derecha o hacia la izquierda para mantener el poste apuntando hacia arriba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlO7WIQHu_7D"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "PIL.Image.fromarray(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9_lskPOey18"
      },
      "source": [
        "El método `environment.step` toma una `acción` en el entorno y devuelve una tupla `TimeStep` que contiene la siguiente observación del entorno y la recompensa por la acción.\n",
        "\n",
        "El método `time_step_spec()` devuelve la especificación de la tupla `TimeStep`. Su atributo \"observación\" muestra la forma de las observaciones, los tipos de datos y los rangos de valores permitidos. El atributo \"recompensa\" muestra los mismos detalles para la recompensa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exDv57iHfwQV"
      },
      "outputs": [],
      "source": [
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxiSyCbBUQPi"
      },
      "outputs": [],
      "source": [
        "print('Reward Spec:')\n",
        "print(env.time_step_spec().reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_lHcIcqUaqB"
      },
      "source": [
        "El método `action_spec()` devuelve la forma, los tipos de datos y los valores permitidos de acciones válidas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bttJ4uxZUQBr"
      },
      "outputs": [],
      "source": [
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJCgJnx3g0yY"
      },
      "source": [
        "En el entorno de Cartpole:\n",
        "\n",
        "- `observación` es una matriz de 4 flotantes:\n",
        "     - la posición y velocidad del carro\n",
        "     - la posición angular y la velocidad del polo\n",
        "- `recompensa` es un valor flotante escalar\n",
        "- `acción` es un número entero escalar con sólo dos valores posibles:\n",
        "     - `0` — \"mover a la izquierda\"\n",
        "     - `1` — \"mover a la derecha\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2UGR5t_iZX-"
      },
      "outputs": [],
      "source": [
        "time_step = env.reset()\n",
        "print('Time step:')\n",
        "print(time_step)\n",
        "\n",
        "action = np.array(1, dtype=np.int32)\n",
        "\n",
        "next_time_step = env.step(action)\n",
        "print('Next time step:')\n",
        "print(next_time_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JSc9GviWUBK"
      },
      "source": [
        "Por lo general, se crean instancias de dos entornos: uno para capacitación y otro para evaluación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7brXNIGWXjC"
      },
      "outputs": [],
      "source": [
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuUqXAVmecTU"
      },
      "source": [
        "El entorno Cartpole, como la mayoría de los entornos, está escrito en Python puro. Esto se convierte a TensorFlow usando el contenedor `TFPyEnvironment`.\n",
        "\n",
        "La API del entorno original utiliza matrices Numpy. El \"TFPyEnvironment\" los convierte en \"Tensores\" para hacerlos compatibles con los agentes y políticas de Tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xp-Y4mD6eDhF"
      },
      "outputs": [],
      "source": [
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9lW_OZYFR8A"
      },
      "source": [
        "## Agente\n",
        "\n",
        "El algoritmo utilizado para resolver un problema de RL está representado por un \"Agente\". TF-Agents proporciona implementaciones estándar de una variedad de \"Agentes\".\n",
        "\n",
        "El agente DQN se puede utilizar en cualquier entorno que tenga un espacio de acción discreto.\n",
        "\n",
        "En el corazón de un agente DQN se encuentra una \"QNetwork\", un modelo de red neuronal que puede aprender a predecir \"QValues\" (rendimientos esperados) para todas las acciones, dada una observación del entorno.\n",
        "\n",
        "Usaremos `tf_agents.networks.` para crear una `QNetwork`. La red constará de una secuencia de capas `tf.keras.layers.Dense`, donde la capa final tendrá 1 salida para cada acción posible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgkdEPg_muzV"
      },
      "outputs": [],
      "source": [
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Define a helper function to create Dense layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu,\n",
        "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# its output.\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z62u55hSmviJ"
      },
      "source": [
        "Ahora use `tf_agents.agents.dqn.dqn_agent` para crear una instancia de un `DqnAgent`. Además de `time_step_spec`, `action_spec` y QNetwork, el constructor del agente también requiere un optimizador (en este caso, `AdamOptimizer`), una función de pérdida y un contador de pasos de números enteros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbY4yrjTEyc9"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0KLrEPwkn5x"
      },
      "source": [
        "## Políticas\n",
        "\n",
        "Una política define la forma en que actúa un agente en un entorno.\n",
        "\n",
        "Normalmente, el objetivo del aprendizaje por refuerzo es entrenar el modelo subyacente hasta que la política produzca el resultado deseado.\n",
        "\n",
        "Objetivos:\n",
        "\n",
        "- El resultado deseado es mantener el poste en equilibrio sobre el carro.\n",
        "- La política devuelve una acción (izquierda o derecha) para cada observación de `time_step`.\n",
        "\n",
        "Los agentes contienen dos políticas:\n",
        "\n",
        "- `agent.policy`: la política principal que se utiliza para la evaluación y la implementación.\n",
        "- `agent.collect_policy`: una segunda política que se utiliza para la recopilación de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwY7StuMkuV4"
      },
      "outputs": [],
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94rCXQtbUbXv"
      },
      "source": [
        "## Métricas y Evaluación\n",
        "\n",
        "La métrica más común utilizada para evaluar una póliza es el rendimiento promedio.\n",
        "\n",
        "El retorno es la suma de las recompensas obtenidas al ejecutar una póliza en un entorno para un episodio.\n",
        "\n",
        "Se ejecutan varios episodios, lo que genera un rendimiento medio.\n",
        "\n",
        "La siguiente función calcula el rendimiento promedio de una póliza, dada la política, el entorno y una cantidad de episodios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bitzHo5_UbXy"
      },
      "outputs": [],
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "# See also the metrics module for standard implementations of different metrics.\n",
        "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLva6g2jdWgr"
      },
      "source": [
        "## Búfer de reproducción\n",
        "\n",
        "Para realizar un seguimiento de los datos recopilados del entorno, utilizaremos [Reverb](https://deepmind.com/research/open-source/Reverb), un sistema de reproducción eficiente, extensible y fácil de usar.\n",
        "\n",
        "Almacena datos de experiencia cuando recopilamos trayectorias y se consume durante el entrenamiento.\n",
        "\n",
        "Este búfer de reproducción se construye utilizando especificaciones que describen los tensores que se almacenarán, que se pueden obtener del agente mediante agent.collect_data_spec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX2zGUWJGWAl"
      },
      "outputs": [],
      "source": [
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_max_length,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length=2,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  replay_buffer.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGNTDJpZs4NN"
      },
      "source": [
        "Para la mayoría de los agentes, `collect_data_spec` es una `named tuple`  llamada `Trayectoria`, que contiene las especificaciones para observaciones, acciones, recompensas y otros elementos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IZ-3HcqgE1z"
      },
      "outputs": [],
      "source": [
        "agent.collect_data_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy6g1tGcfRlw"
      },
      "outputs": [],
      "source": [
        "agent.collect_data_spec._fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBc9lj9VWWtZ"
      },
      "source": [
        "## Entrenando al agente (Implemente su loop de entrenamiento aqui!)\n",
        "\n",
        "Deben suceder dos cosas durante el ciclo de entrenamiento:\n",
        "\n",
        "- recopilar datos del medio ambiente\n",
        "- utilizar esos datos para entrenar las redes neuronales del agente\n",
        "\n",
        "Este ejemplo también evalúa periódicamente la política e imprime la puntuación actual.\n",
        "\n",
        "Lo siguiente tardará 5-10 minutos en ejecutarse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pTbJ3PeyF-u"
      },
      "outputs": [],
      "source": [
        "#Implementar Aqui la politica de entrenamiento\n",
        "from tf_agents.replay_buffers import TFUniformReplayBuffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
        "\n",
        "# Asegúrate de que todas las importaciones necesarias estén aquí\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "# ... (resto de tu configuración y definiciones previas)\n",
        "\n",
        "# Configurar el buffer de repetición\n",
        "replay_buffer = TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)\n",
        "\n",
        "# ... (code for agent configuration and functions)\n",
        "\n",
        "\n",
        "# Bucle de entrenamiento\n",
        "for _ in range(num_iterations):\n",
        "    # Recopilar unos pocos pasos usando collect_policy y guardar en el buffer\n",
        "    for _ in range(collect_steps_per_iteration):\n",
        "        collect_step(train_env, agent.collect_policy)\n",
        "\n",
        "    # Muestrear un lote de datos del buffer y actualizar la red del agente\n",
        "    # Asegúrate de que 'iterator' esté definido y configurado correctamente aquí\n",
        "    experience, unused_info = next(iterator)\n",
        "    train_loss = agent.train(experience).loss\n",
        "\n",
        "    step = agent.train_step_counter.numpy()\n",
        "\n",
        "    if step % log_interval == 0:\n",
        "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "    if step % eval_interval == 0:\n",
        "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-LWCdbbOIC"
      },
      "source": [
        "### Gráficos\n",
        "\n",
        "Utilice `matplotlib.pyplot` para registrar cómo mejoró la política durante el entrenamiento.\n",
        "\n",
        "Una iteración de `Cartpole-v0` consta de 200 pasos de tiempo. El entorno ofrece una recompensa de \"+1\" por cada paso que el poste se mantenga arriba, por lo que el retorno máximo para un episodio es 200. Los gráficos muestran que el retorno aumenta hacia ese máximo cada vez que se evalúa durante el entrenamiento. (Puede ser un poco inestable y no aumentar de forma monótona cada vez)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxtL1mbOYCVO"
      },
      "outputs": [],
      "source": [
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=250)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7-XpPP99Cy7"
      },
      "source": [
        "### Videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pGfGxSH32gn"
      },
      "source": [
        "Los gráficos son bonitos.\n",
        "\n",
        "Pero lo más emocionante es ver a un agente realizando una tarea en un entorno.\n",
        "\n",
        "Primero, cree una función para insertar videos en el notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULaGr8pvOKbl"
      },
      "outputs": [],
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_PH-pX4Pr5"
      },
      "source": [
        "Ahora repite algunos episodios del juego Cartpole con el agente. El entorno Python subyacente (el que está \"dentro\" del contenedor del entorno TensorFlow) proporciona un método `render()`, que genera una imagen del estado del entorno. Estos se pueden recopilar en un vídeo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owOVWB158NlF"
      },
      "outputs": [],
      "source": [
        "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
        "  filename = filename + \".mp4\"\n",
        "  with imageio.get_writer(filename, fps=fps) as video:\n",
        "    for _ in range(num_episodes):\n",
        "      time_step = eval_env.reset()\n",
        "      video.append_data(eval_py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = eval_env.step(action_step.action)\n",
        "        video.append_data(eval_py_env.render())\n",
        "  return embed_mp4(filename)\n",
        "\n",
        "create_policy_eval_video(agent.policy, \"trained-agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "povaAOcZygLw"
      },
      "source": [
        "POr diversion, compara al agente entrenado (arriba) con un agente que se mueve al azar. (No funciona tan bien)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJZIdC37yNH4"
      },
      "outputs": [],
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())\n",
        "create_policy_eval_video(random_policy, \"random-agent\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "94rCXQtbUbXv",
        "NLva6g2jdWgr"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
